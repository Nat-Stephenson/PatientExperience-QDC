<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>PatientExperience-QDC website</title>
<link>https://nat-stephenson.github.io/PatientExperience-QDC/updates.html</link>
<atom:link href="https://nat-stephenson.github.io/PatientExperience-QDC/updates.xml" rel="self" type="application/rss+xml"/>
<description>Website for the NHS England funded Patient Experience Qualitative Data Categorisation project</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Tue, 09 Jan 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Project conclusion</title>
  <dc:creator>YiWen Hon</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-24-01-29/</link>
  <description><![CDATA[ 




<p>This project concluded in December 2023, but all the code is available on GitHub, and the API will continue to be hosted by The Strategy Unit throughout 2024 at least.</p>
<p>We presented on pxtextmining and the dashboard at the NHS-R conference; links to the recordings are available below.</p>
<ul>
<li><a href="https://youtu.be/3uSbLT7ISPc?si=i38cktknYHrodojb&amp;t=7510">experiencesdashboard at the NHS-R Conference 2023</a></li>
<li><a href="https://www.youtube.com/watch?v=8jcPwD8x8WE&amp;t=16293s">pxtextmining at the NHS-R Conference 2023</a></li>
</ul>



 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-24-01-29/</guid>
  <pubDate>Tue, 09 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Ensembling models</title>
  <dc:creator>YiWen Hon</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-13/</link>
  <description><![CDATA[ 




<p>We found that there are three different model architectures / algorithms that produced a reasonable performance for predicting the themes: XGBoost, Distilbert, and Support Vector Machine. We decided to combine the three models together, in order to build upon their individual strengths and improve overall performance. In data science terminology, this is called “ensembling”.</p>
<p>We tried three different methods of ensembling:</p>
<ul>
<li>Taking all the predicted labels from each model. If Models A and B predicted the label “Safety &amp; security” and Model C predicted the label “Feeling safe”, we would take both “Safety &amp; security” and “Feeling safe” as the predicted labels for the text. This method resulted in a higher recall score but lower precision.</li>
<li>Taking predicted labels only if at least two models predicted it. This is called “Hard voting” in data science terminology. If Models A and B predicted the label “Safety &amp; security” and Model C predicted the label “Feeling safe”, we would take only “Safety &amp; security” as the predicted label for the text. This is because “Feeling safe” was predicted by only one model. This method did not increase performance.</li>
<li>Taking the predicted probabilities from each model and averaging them. This is called “Soft voting” in data science terminology. For example, if Models A, B, and C predicted the probability of the text belonging to the class “Feeling safe” as 0.2, 0.9 and 0.8 respectively, the average would be 0.63. This method worked the best, particularly when the threshold for accepting a predicted label was lowered to 0.3. This offered the best balance between precision and recall.</li>
</ul>
<p>We have opted for method 3, as it offered the best balance between improving precision and recall. However, it also takes a longer time as we need to wait for all three models to make their predictions before we combine them. We are using this method with the slower “wait” API.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-13/ensemble.png" class="img-fluid figure-img"></p>
<figcaption>Diagram showing how models are ensembled</figcaption>
</figure>
</div>



 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-13/</guid>
  <pubDate>Thu, 12 Oct 2023 23:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-13/ensemble.png" medium="image" type="image/png" height="152" width="144"/>
</item>
<item>
  <title>Productionising an LLM with limited hardware resources</title>
  <dc:creator>YiWen Hon</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-12/</link>
  <description><![CDATA[ 




<p>For this project, we tried several different machine learning algorithms. The best performing algorithms were the Support Vector Classifier, Gradient Boosted Decision Trees (XGBoost), and Distilbert. For the multilabel categories, we found that these three models were largely comparable to one another but that they produced the best results when combined. The Distilbert model outperformed the others for predicting text sentiment (i.e., positive/negative/neutral).</p>
<p>The Distilbert model is a transformer-based model, which means that it is very slow and requires a lot of computation power. When we tried to deploy it on our RStudio Connect server, we found that it was consuming 100% of the server’s resources! This was not sustainable and we needed to find another way to make this model available.</p>
<p>The solution that we have come up with is to use Azure Container Instances to host the model. Now, when users send their data for predictions, they get a unique results URL to check. A Docker container with their data is started up, which also installs the pxtextmining package and loads the models. This can take a few minutes, as a fresh instance is set up every time. The Docker container then makes the predictions based on the data, and saves the predictions (without the text). The process of making the predictions can be quite slow, approximately 10 minutes for 5,000 comments. After predictions are made, the original data is deleted, and the predicted categories are returned via the unique results URL. The container is then shut down completely.</p>
<p>We adopted this approach because it is more cost effective. We only pay for the length of time that the container instance is running - around 5-30 minutes, depending on the number of comments submitted. However, this does mean that it is a slower process overall as the container instance needs to be started up from scratch every time. It therefore makes more sense to use this for processing many comments at once. It’s also a slightly more complicated API to use, but we have provided guidance and example code to aid our end users.</p>
<p>We are still maintaining the API which uses the lightweight sklearn model. The performance of the model here is less good, but we think it’s important to maintain a ‘quick and easy’ solution as well as the more performant but much slower approach.</p>
<p>Do you have experience with deploying slow, resource-intensive models with limited resources? What were your solutions?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-12/slow_API.png" class="img-fluid figure-img"></p>
<figcaption>Diagram showing Slow API structure</figcaption>
</figure>
</div>



 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-12/</guid>
  <pubDate>Wed, 11 Oct 2023 23:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-10-12/slow_API.png" medium="image" type="image/png" height="120" width="144"/>
</item>
<item>
  <title>Experience Dashboard at Version 0.7.0: A data exploration tool</title>
  <dc:creator>Oluwasegun Apejoye</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/dashboard-update-23-07-18/</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Our first update on the dashboard was based on version 0.6 of the dashboard. And we noted then that we would be rolling out some features into the dashboard. We are glad to say that we implemented some changes and added new features, in version 0.7 series of the dashboard, which we believe have added some elegance to the dashboard and improved its usefulness as a data exploration and visualisation tool to help clinicians, ward managers, and general users identify the pattern in free text data and to access the underlying comments to explore explanations for any pattern seen on the visual.</p>
</section>
<section id="new-or-updated-features" class="level2">
<h2 class="anchored" data-anchor-id="new-or-updated-features">New or updated features</h2>
<p>In this section, I will provide a high-level overview of the changes we made and the additional features we introduced.</p>
<p><strong>Move from single-labelling to multi-labelling of comments:</strong> This is the main highlight of all the changes that we introduced. Multi-tagging of patient comments is a crucial part of this phase of the patient experience data categorisation project. The dashboard has always supported single labelled categories but we have now improved the dashboard to support multi-labelled sub-categories and categories (also called super-category in this post). The data format of the dashboard underlining data has also changed to accommodate these changes.</p>
<p><strong>Adoption of the NHS Identity:</strong> We made every effort to change the general dashboard theme and visualisation colours to follow the <a href="https://www.england.nhs.uk/nhsidentity/">NHS Identity</a>. This was done to provide consistency between the dashboard and other NHS-related dashboards since the dashboard is targeted at NHS-related organisations. Note: This project is still open source and can be used by anyone as far as you follow the <a href="https://github.com/The-Strategy-Unit/experiencesdashboard/blob/documentation/LICENSE.md">MIT Licence</a> under which this work is distributed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/landing_page.png"><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/dashboard-update-23-07-18/images/landing_page.png" class="img-fluid figure-img" alt="New Landing Page"></a></p>
<figcaption>Fig 1. New Landing Page</figcaption>
</figure>
</div>
<p><strong>Update SPC Chart of the FFT Scores:</strong> We used the <a href="https://github.com/nhs-r-community/NHSRplotthedots">NHSR plotthedots package</a> to draw the SPC (statistical process control) chart. The now follows the principles laid out in the NHSE programme, <a href="https://www.england.nhs.uk/publication/making-data-count/">‘Making Data Count’</a>, and allows users to easily see change points, and identify when rules are breached.</p>
<p><strong>Use Upset plot instead of HeatMap to visualise inter-relationships between sub-categories</strong>: Using an upset plot allows users to see the relationships and associations between the large sub-categories (over 40 different sub-categories) present in the data. <a href="../../posts/upset_plot-23-07-18/index.html">Read more here</a></p>
<p><strong>QDC Framework</strong>: The landing page now contains a high-level overview of the data categorisation framework developed as part of this project and implemented to assign the sub-categories to the free text comments. see the <a href="../../framework/index.html">framework</a> for more details.</p>
<p><strong>Visualise trends in sub-categories</strong>: We added a plot to visualise the distribution of comments over time.</p>
<p><strong>Implement the Pxtextming API</strong>: We implemented the <a href="../../posts/update-23-05-31/index.html">pxtextmining API</a> for machining learning tasks (predicting the sub-categories). Using the API was a more efficient way to interact with the Python machine learning model compared to the previous implementation which requires installing the R version of the pxtextmining package, <code>pxtextmineR</code>, <code>reticulate</code> and downloading the actual Python models needed for the prediction.</p>
<p><strong>Download complex comments</strong> <strong>and other data</strong>: We added the functionality to download the complex comment for further analysis. Also, users can now download the underline data serving the visualisation they interact with. They can also download all the historical data that serve the dashboard.</p>
<p><strong>Filter with Demographic features</strong>: Users can now drill down into the data using three demographic features such as age, sex, and ethnicity. This alongside the location and date filter will allow users to drill down the data to their target population and analyse further.</p>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future work</h2>
<p>As we progress, we eagerly embrace the invaluable feedback from the dashboard users. Our commitment to agility remains steadfast, allowing us to efficiently incorporate and implement the essential updates inspired by users’ feedback, to make sure the dashboard consistently meets their evolving needs. We’ll continue to refine and enrich the dashboard’s features to achieve optimal performance and user satisfaction.</p>


</section>

 ]]></description>
  <category>news</category>
  <category>dashboard</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/dashboard-update-23-07-18/</guid>
  <pubDate>Mon, 17 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/dashboard-update-23-07-18/images/landing_page.png" medium="image" type="image/png" height="73" width="144"/>
</item>
<item>
  <title>Using an upset plot to explore the relationship between large categorical data</title>
  <dc:creator>Oluwasegun Apejoye</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/upset_plot-23-07-18/</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>An upset plot is a great visualisation tool that can be used to show and understand the relationships and associations between large categories (called sets) in data. It shows the number of elements (number of comments in our case) the set (sub-categories) have in common and visualise it nicely using a matrix layout and a series of bar charts. The matrix layout enables the effective representation of associated data, such as the number of elements in the aggregates and intersections.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/annotated_Upset_plot.jpg"><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/upset_plot-23-07-18/images/annotated_Upset_plot.jpg" class="img-fluid figure-img" alt="An annotated upset plot"></a></p>
<figcaption>Upset plot (click on the image to expand it)</figcaption>
</figure>
</div>
<section id="how-is-it-plotted" class="level3">
<h3 class="anchored" data-anchor-id="how-is-it-plotted">How is it plotted?</h3>
<p>To plot an upset plot the data has to be prepared. Using our use case as an example, we prepared the data in this way:</p>
<ul>
<li><p>Every record (row) represents one comment</p></li>
<li><p>Each column represents a label with a yes/no flag as to whether or not that comment was assigned the label. It is coded 1 for “yes” and 0 for “no”,</p></li>
<li><p>All the label names are used as the attribute on the plot.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/upset-data.jpg"><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/upset_plot-23-07-18/images/upset-data.jpg" class="img-fluid figure-img" alt="sample data for upset plot"></a></p>
<figcaption>Sample data for upset plot (click on the image to expand it)</figcaption>
</figure>
</div>
</section>
<section id="why-upset-plot-and-how-should-it-be-used" class="level3">
<h3 class="anchored" data-anchor-id="why-upset-plot-and-how-should-it-be-used">Why Upset plot and how should it be used?</h3>
<p>In this project, we assign labels (sub-categories) to free text comments in a way that one comment can have 1 or more labels assigned to it. Upset plots provide a way to show the relationship between the assigned labels simultaneously. By exploring this pattern we can identify groups of categories that are mostly discussed together in a single comment. This plot can guide us as we explore answers to questions such as this:</p>
<p><em>Do most comments about staff “competence &amp; training” also relate to the “provision of medical equipment”?</em></p>
<p>This plot is used in the dashboard as a visual guide to help users as they explore the free text comments. The plot can be used to gain insight into the sub-categories that people might talk about more frequently within the same comments. However, the fact that people talk about a group of sub-categories in the same comment doesn’t mean those sub-categories have a relationship in reality, and that is why we provide functionality within the dashboard for users to further explore the free text comments. The comments should be read to explore explanations for any pattern seen on the visual and to carry out any additional qualitative analysis as required.</p>


</section>
</section>

 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/upset_plot-23-07-18/</guid>
  <pubDate>Mon, 17 Jul 2023 23:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/upset_plot-23-07-18/images/annotated_Upset_plot.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>pxtextmining API: enabling access to project outputs</title>
  <dc:creator>YiWen Hon</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-05-31/</link>
  <description><![CDATA[ 




<p>Our latest major project milestone is the release of a model API. This blog post will explain what this is, how we have achieved it, and what our next steps are.</p>
<section id="what-is-an-api" class="level2">
<h2 class="anchored" data-anchor-id="what-is-an-api">What is an API?</h2>
<p>API stands for “application programming interface”. Most of the digital services and platforms that we use have an API. It is essentially a gateway that allows access to information. For example, when you ask an Alexa or Siri what the weather is, it will communicate with a weather service API to obtain the information for you. When you get an email notification on your phone, this is because your email service provider is sending a notice via its API, to your phone.</p>
<p>Still confused? <a href="https://youtu.be/s7wmiS2mSXY">This video</a> may help.</p>
<p>The pxtextmining API allows people to utilise the machine learning models that we have trained for the project to label new text. It makes the model accessible over the internet, without any installation or downloads required. All that’s needed is a few lines of code. Examples are available on the <a href="https://The-Strategy-Unit.github.io/pxtextmining/reference/API/API/">pxtextmining documentation website</a>.</p>
</section>
<section id="why-did-we-do-it" class="level2">
<h2 class="anchored" data-anchor-id="why-did-we-do-it">Why did we do it?</h2>
<p>The API was created to separate our R/Shiny-based experiencesdashboard frontend and the Python-based pxtextmining backend. This independence means more flexibility in the development of each. This also means that some participating trusts can utilise the model only, if they wish, and integrate it into their existing visualisation tools/dashboards for exploring patient feedback data.</p>
<p>We also recognise that although the code and models for pxtextmining are available openly on github, many organisations may not have the technical resources, time, or capability to run it themselves. Making the model available via an API reduces the barrier to entry and ensures that the project outputs are more accessible.</p>
</section>
<section id="how-did-we-do-it" class="level2">
<h2 class="anchored" data-anchor-id="how-did-we-do-it">How did we do it?</h2>
<p>The API is built using the open source <a href="https://fastapi.tiangolo.com/lo/">Python FastAPI package</a>. Users of the API must format their data in JSON format, and submit this to the API endpoint via a POST request. This then gets cleaned and passed through the machine learning model, and the outputted labels are returned, also in JSON format. We utilised the <a href="https://www.uvicorn.org/">Uvicorn</a> library to test the API locally, and have ensured that the API is covered by unit tests. We are hosting the API on the Strategy Unit’s Rstudio Connect server. Deploying it was very simple, luckily! The diagram below shows how the API works.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-05-31/px_diagram.jpg" class="img-fluid figure-img"></p>
<figcaption>Diagram of API</figcaption>
</figure>
</div>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s next?</h2>
<p>The API currently uses a <a href="https://youtu.be/efR1C6CvhmE">support vector classifier</a> model which is quite small and fast. Even so, we have to balance the load on the server with waiting times, limiting the number of requests to 1000 at a time to prevent timeouts. We also have a <a href="https://youtu.be/SZorAJ4I-sA">transformer-based model</a> which is very large and slow, taking several minutes to make predictions from text, which we have not implemented an API for. Our next step is to figure out how to do this without completely taking over the server’s resources, and impacting too much on the user experience.</p>
<p>We also want to make sure that participating trusts are able to make use of the API, and help demystify it. Communication is key, and blog posts like this will hopefully help. We are also providing example scripts showing how to create API queries in R and Python, and have built a <a href="https://connect.strategyunitwm.nhs.uk/content/0049176a-56d7-40c9-bf07-26262e9ee53c">simple website</a> that shows the API and the model in action.</p>
<p>The model that is used in the API is also still a work in progress, we know we have a lot of work to do to improve performance. But now that the technical infrastructure is in place, it should be relatively easy to update with new models, when we train them. If you’re interested in using the API, do get in touch!</p>


</section>

 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-05-31/</guid>
  <pubDate>Tue, 30 May 2023 23:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-05-31/px_diagram.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Machine learning update</title>
  <dc:creator>YiWen Hon</dc:creator>
  <link>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-03-13/</link>
  <description><![CDATA[ 




<p>We’ve had a chance to play around with a few different model architectures now and the main finding is that so far, as in Phase 1 and other <a href="https://www.sciencedirect.com/science/article/pii/S1386505621002689?via%3Dihub">similar projects</a>, Support Vector Classifier is performing the best. We are currently prototyping using only some of the final dataset, and focusing on predicting 1 of 13 overarching ‘major categories’. These are divided into a further 50+ subcategories, which our models are not yet trained to detect, but this will be the next step, once the framework is finalised.</p>
<p>We have also tried out Distilbert uncased, which is performing well, if not slightly better, than SVC. However, it’s taking a long time to train (around 6 hours on average) and is very large, around 1 GB in size. It also takes a long time to make predictions, around 1 minute per 1000 comments. When we have the final dataset we’ll have a clearer picture of which model will be best suited for the needs of the project.</p>
<p>The latest development is that we’re able to combine the text with the ‘question type’ feature. Most of the questions asked by trusts can be divided into one of three main types:</p>
<ul>
<li><p>What did we do well?</p></li>
<li><p>What could we do better?</p></li>
<li><p>Any other comments / no specific question</p></li>
</ul>
<p>Including the question type with the text of the comment has improved performance slightly. The way this has been done with BERT can be seen in the diagram below. We tried both of the methods found in <a href="https://stackoverflow.com/a/73576407/17852099">this stackoverflow post</a>, which produced similar results. We went for the concatenated layers, however, as this means the model architecture is more flexible and can be adapted to include other features as well, if necessary.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-03-13/model_architecture.png" class="img-fluid figure-img"></p>
<figcaption>Distilbert model with categorical question type feature</figcaption>
</figure>
</div>



 ]]></description>
  <category>news</category>
  <guid>https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-03-13/</guid>
  <pubDate>Mon, 13 Mar 2023 00:00:00 GMT</pubDate>
  <media:content url="https://nat-stephenson.github.io/PatientExperience-QDC/posts/update-23-03-13/model_architecture.png" medium="image" type="image/png" height="107" width="144"/>
</item>
</channel>
</rss>
